{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import datasets, layers, models \n",
    "from keras.datasets import fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images.astype('float32') / 255 \n",
    "test_images = test_images.astype('float32') / 255 \n",
    "\n",
    "from keras.utils import to_categorical \n",
    "train_labels = to_categorical(train_labels) \n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " original_img (InputLayer)   [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 26, 26, 16)        160       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 8, 8, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 6, 32)          9248      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 16)          4624      \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 16)               0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18,672\n",
      "Trainable params: 18,672\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 14:50:45.655938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "encoder_input = tf.keras.Input(shape=(28, 28, 1), name='original_img') \n",
    "\n",
    "x = layers.Conv2D(16, 3, activation='relu')(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.Conv2D(16, 3, activation='relu')(x) \n",
    "\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = tf.keras.Model(encoder_input, encoder_output, name='encoder') \n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoded_img (InputLayer)    [(None, 16)]              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 1)           0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 6, 6, 16)         160       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 8, 8, 32)         4640      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 26, 26, 16)       4624      \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 28, 28, 1)        145       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,569\n",
      "Trainable params: 9,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_input = tf.keras.Input(shape=(16,), name='encoded_img') \n",
    "\n",
    "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
    "x = layers.Conv2DTranspose(16, 3, activation='relu')(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation='relu')(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation='relu')(x) \n",
    "\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation='relu')(x)\n",
    "decoder = tf.keras.Model(decoder_input, decoder_output, name='decoder') \n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " img (InputLayer)            [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 16)                18672     \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 28, 28, 1)         9569      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,241\n",
      "Trainable params: 28,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder_input = tf.keras.Input(shape=(28, 28, 1), name='img')\n",
    "encoded_img = encoder(autoencoder_input)\n",
    "decoded_img = decoder(encoded_img)\n",
    "autoencoder = tf.keras.Model(autoencoder_input, decoded_img, name='autoencoder')\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 57s 118ms/step - loss: 0.0483 - val_loss: 0.0347\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 59s 126ms/step - loss: 0.0312 - val_loss: 0.0292\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 50s 106ms/step - loss: 0.0281 - val_loss: 0.0269\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 51s 110ms/step - loss: 0.0266 - val_loss: 0.0259\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 50s 107ms/step - loss: 0.0256 - val_loss: 0.0251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe508c66100>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(train_images, train_images, epochs=5, batch_size=128, shuffle=True, validation_data=(test_images, test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and the actual image looks like\n",
      "1/1 [==============================] - 0s 232ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEOCAYAAAApP3VyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAe00lEQVR4nO3df2zV1f3H8del0NtSbi+U2t4WSq0bxk02J+hgzmFZYmOzEJGZOE0W+MfoRBJCjBkzi93+oGoy4h8o+5EFNRuTbVFnplNrtEXHWLDBwcAgziIFWkor3FtauKX08/1j6/22tvece3vvPb23fT6Sm9j7urf3cHo/b9+9vZ/39Xme5wkAAMCRGZO9AAAAML3QfAAAAKdoPgAAgFM0HwAAwCmaDwAA4BTNBwAAcIrmAwAAOEXzAQAAnKL5AAAATs2c7AV80dDQkE6fPq1AICCfzzfZywGmJc/z1Nvbq8rKSs2YkRu/o1A7gMmVVN3wMuSZZ57xrr76as/v93tLly719uzZk9D92tvbPUlcuHDJgkt7e3umSsS4Jlo3PI/awYVLtlwSqRsZeeVj9+7d2rRpk5599ll9+9vf1q9+9SvV19fryJEjWrRokfG+gUBAktTe3q7i4uJMLA+ARSQSUVVVVex4dCGVuiFRO4DJlkzd8Hle+j9Ybvny5Vq6dKl27NgRu+4rX/mK1qxZo8bGRuN9I5GIgsGgwuEwBQSYJJNxHKZSNyRqBzDZkjkG0/7H3IGBAbW2tqqurm7U9XV1ddq7d++Y20ejUUUikVEXANNLsnVDonYAuSztzUd3d7euXLmi8vLyUdeXl5ers7NzzO0bGxsVDAZjl6qqqnQvCUCWS7ZuSNQOIJdl7G3sX3y3ued5474DfcuWLQqHw7FLe3t7ppYEIMslWjckageQy9L+htPS0lLl5eWN+W2lq6trzG81kuT3++X3+9O9DAA5JNm6IVE7gFyW9lc+8vPztWzZMjU1NY26vqmpSbfccku6Hw7AFEDdAKaXjJxqu3nzZv3whz/UTTfdpG9961v69a9/rRMnTujBBx/MxMMBmAKoG8D0kZHm45577lFPT49+/vOfq6OjQ0uWLNHrr7+u6urqTDwcgCmAugFMHxmZ85EKztUHJl8uHoe5uGZgKpnUOR8AAAAmNB8AAMApmg8AAOAUzQcAAHCK5gMAADhF8wEAAJyi+QAAAE7RfAAAAKdoPgAAgFM0HwAAwCmaDwAA4BTNBwAAcIrmAwAAOEXzAQAAnKL5AAAATtF8AAAAp2g+AACAUzQfAADAKZoPAADgFM0HAABwiuYDAAA4RfMBAACcovkAAABO0XwAAACnaD4AAIBTNB8AAMApmg8AAOAUzQcAAHCK5gMAADiV9uajoaFBPp9v1CUUCqX7YQBMIdQNYHqZmYlvev311+vtt9+OfZ2Xl5eJhwEwhVA3gOkjI83HzJkz+a0FQFKoG8D0kZH3fBw7dkyVlZWqqanRD37wA3366adxbxuNRhWJREZdAEw/ydQNidoB5LK0Nx/Lly/XCy+8oDfffFO/+c1v1NnZqVtuuUU9PT3j3r6xsVHBYDB2qaqqSveSAGS5ZOuGRO0AcpnP8zwvkw/Q19enL33pS3r00Ue1efPmMXk0GlU0Go19HYlEVFVVpXA4rOLi4kwuDUAckUhEwWBw0o5DW92QqB1AtkmmbmTkPR8jFRUV6Wtf+5qOHTs2bu73++X3+zO9DAA5xFY3JGoHkMsyPucjGo3qo48+UkVFRaYfCsAUQd0AspfneXEviUp78/HII4+opaVFbW1t+uc//6m7775bkUhE69atS/dDAZgiqBvA9JL2P7ucPHlS9957r7q7u3XVVVdpxYoV2rdvn6qrq9P9UACmCOoGML2kvfl48cUX0/0tAUxx1A1geuGzXQAAgFM0HwAAwCmaDwAA4BTNBwAAcCrjQ8aAbDQ0NGTMfT5fSnkiBgcHjfnMmebD0zR6XJLmz5+f9JqAXDdy6u14ent7jXlpaWk6lzMpbHvQ2dlpzG17UFRUNO71ydRFXvkAAABO0XwAAACnaD4AAIBTNB8AAMApmg8AAOAUzQcAAHCK5gMAADhF8wEAAJxiyBgmxPO8lO5vG0ZjGwR0+PBhY7506VJjnp+fb8xdsA0Rs/nrX/9qzNetW5fS9wdy0R//+EdjHolEjLmtdpSXlxvz6upqY56Xl2fME3HmzBlj/vHHHxvz06dPG3PbHixevNiYJ4JXPgAAgFM0HwAAwCmaDwAA4BTNBwAAcIrmAwAAOEXzAQAAnKL5AAAATjHnAxlhm+Nh869//cuYt7S0GPOTJ08a87vvvjvpNaVbX1+fMd+/f78xnzt3bhpXA+SGI0eOGHPbnA/bjJ+PPvrImF9//fXGfNmyZca8uLjYmEtSd3e3MbftwYcffmjMBwYGjHlVVZUxv+aaa8a9/sqVK8b7jcQrHwAAwCmaDwAA4BTNBwAAcIrmAwAAOEXzAQAAnKL5AAAATtF8AAAAp5jzgQnxPM+Y2+Z8fPbZZ8bcNudjwYIFxvzw4cPG/G9/+5sxnz9/vjG/ePGiMZekmpoaY3727FljHolEjPmiRYuM+Z133mnMgVz0xhtvGPOjR48a83gzKoadOnXKmNuO/a6uLmM+e/ZsY57I9+jp6THmtvo3NDRkzI8fP27Mv/GNb4x7fTQaNd5vpKRf+dizZ49Wr16tyspK+Xw+vfLKK6Nyz/PU0NCgyspKFRYWqra21roRAKY26gaAkZJuPvr6+nTDDTdo+/bt4+ZPPfWUtm3bpu3bt2v//v0KhUK6/fbb1dvbm/JiAeQm6gaAkZL+s0t9fb3q6+vHzTzP09NPP63HHntMa9eulSQ9//zzKi8v165du/TAAw+MuU80Gh31Uo3tpWYAuSfddUOidgC5LK1vOG1ra1NnZ6fq6upi1/n9ft12223au3fvuPdpbGxUMBiMXWwz5QFMLROpGxK1A8hlaW0+Ojs7JUnl5eWjri8vL49lX7RlyxaFw+HYpb29PZ1LApDlJlI3JGoHkMsycrbLF8908Dwv7tkPfr9ffr8/E8sAkEOSqRsStQPIZWl95SMUCknSmN9Wurq6xvxWAwASdQOYjtL6ykdNTY1CoZCampp04403SpIGBgbU0tKiJ598Mp0PhQyzzfGYMcPctw4MDBjzP/3pT8a8oKDAmPf39xtz25sPbf8+23nwtvtL0sGDB415dXW1MS8pKTHmly9ftq4hF1A3MNLHH39szE+cOGHM482gGGY7dtva2ox5X1+fMb9w4YIxt9UWSZo1a5Yxt80asdXH4uJi6xpM4u1hInVxWNLNx4ULF/TJJ5/Evm5ra9OHH36okpISLVq0SJs2bdLWrVu1ePFiLV68WFu3btXs2bN13333JftQAKYI6gaAkZJuPj744AOtWrUq9vXmzZslSevWrdNzzz2nRx99VBcvXtRDDz2kc+fOafny5XrrrbcUCATSt2oAOYW6AWCkpJuP2tpa40srPp9PDQ0NamhoSGVdAKYQ6gaAkfhgOQAA4BTNBwAAcIrmAwAAOEXzAQAAnMrIhNPpznaus2lqY6Lfw8b2GLZzzW1zPGz+/Oc/G/MFCxYY89mzZxvzY8eOGfNLly4Z84qKCmM+ODhozBPZnzlz5hjz/Px8Yx4Oh4257d9om7Vie3wgE0wj8yWpubnZmJ8+fdqY245N23HZ1dVlzG3rt9WuefPmGXNJKioqMuaVlZUpP4aJbdZJOuZ88MoHAABwiuYDAAA4RfMBAACcovkAAABO0XwAAACnaD4AAIBTNB8AAMAp5nyMI9U5HYnM8bBJ9Xtkeo7He++9Z8xPnjxpzFesWGHMr1y5YszPnTtnzEtLS1PKbef69/b2GnPJPivExvYz7O/vN+Znz5415rZZK8BEnDlzxpg/99xzxrynpyelx7fVtgsXLhhz27Hd19dnzG21JRG2OR22+lhQUGDMbbUjEokY83i1yVazRuKVDwAA4BTNBwAAcIrmAwAAOEXzAQAAnKL5AAAATtF8AAAAp2g+AACAU8z5GEeqMzZsc0JseSJrsOWpzvFoamoy5gcPHjTm11xzjTG3zaCw7ZHtPPWqqipjHg6Hjblt/2bPnm3MJenSpUvGPNV5MjZvv/22MV+3bl1K3x/Tk21ORmtrqzE/cuSIMbfNx2lvbzfmc+fONeYDAwPG3Hbc5efnG/NAIGDMCwsLjXkibPXPNgfEVptss1aY8wEAAHIOzQcAAHCK5gMAADhF8wEAAJyi+QAAAE7RfAAAAKdoPgAAgFNTcs5HInM0MinVGR3p0Nvba8xtczz6+vqM+ZIlS4x5JBIx5hcvXjTmXV1dxtx2rr1tj23/Ppu8vDzrbfx+vzGfOdN8+M2ZM8eY22aRvPvuu8acOR/Tk+3YPHnypDHv6Ogw5h988IExv3z5sjG3Hbu2GTvRaDSl3Ma2flvttc1JkaRZs2YltaYvOnXqlDG37UFxcbExjzcrxbY3IyX9yseePXu0evVqVVZWyufz6ZVXXhmVr1+/Xj6fb9RlxYoVyT4MgCmEugFgpKSbj76+Pt1www3avn173Nvccccd6ujoiF1ef/31lBYJILdRNwCMlPSfXerr61VfX2+8jd/vVygUmvCiAEwt1A0AI2XkDafNzc0qKyvTtddeq/vvv9/49/toNKpIJDLqAmD6SaZuSNQOIJelvfmor6/X73//e73zzjv6xS9+of379+u73/1u3De4NDY2KhgMxi62DwQDMPUkWzckageQy9J+tss999wT++8lS5bopptuUnV1tV577TWtXbt2zO23bNmizZs3x76ORCIUEWCaSbZuSNQOIJdl/FTbiooKVVdX69ixY+Pmfr/fekoigOnFVjckageQyzLefPT09Ki9vV0VFRVJ3W9oaEhDQ0PjZrb5Bi7maKSiv7/fepuzZ88a8+PHjxvzEydOGPOCggJjPm/ePGPe09NjzM+dO2fM450nPuzSpUvG3PYcaGtrM+a289Hnzp1rzG1zRiT7Gq9cuWLMbfMMbPe3navf2dkZN7PNKsi0idYNSQqHw3Fn/dieV7bnZWVlpTFPZP5LKkw/s2F///vfjXl7e7sxtx27tuelbYaOrT6fP3/emNvY1m+bcWH7Gcb7/9Iw27GTyIwh27FtmwGU6qwT26yXePUzmTkfSTcfFy5c0CeffBL7uq2tTR9++KFKSkpUUlKihoYGff/731dFRYWOHz+un/zkJyotLdVdd92V7EMBmCKoGwBGSrr5+OCDD7Rq1arY18N/c123bp127NihQ4cO6YUXXtD58+dVUVGhVatWaffu3QoEAulbNYCcQt0AMFLSzUdtba1xfPmbb76Z0oIATD3UDQAj8cFyAADAKZoPAADgFM0HAABwiuYDAAA4lfE5HxM1Y8YM65yEeGznUXd3dxvzixcvppTbZgmYBicNu3DhgjG3zZkIBoPG3HYe+eeff27MbXs8a9aslO5fVFRkzG1zSmzzGhYsWGDMbbMCEjlXv6SkxJjb5gHYfga2c/07Ojom/Pi25182u3z5ctyfv23+y2effWbMr776amNeWFhozG3PG9u+J/L5NQcPHjTmthkPtuf+zJnm/22cOnXKmNvqo20PbLntM4Fs/18ZHBw05ja22mc7biX7Hqc6B+mqq64y5rZZLvH2KJm945UPAADgFM0HAABwiuYDAAA4RfMBAACcovkAAABO0XwAAACnaD4AAIBTWTvnw+TIkSPG/MyZM8Y8Ly8vpfvbzmW2naNtOw9ckubOnWvMw+GwMW9vbzfmpg/5kuznkc+fP9+Y2+aI2GZc2PbYdq687dNQ582bZ8w7OzuNeTrY9tD2PO3v7zfmtnk0pu8/0Rk72WBoaEhDQ0PjZrYZP8ePHzfmJ06cMOapztCwHZfx/l0j2eZg2I6NRGaJmNhmmfT09Bhz23PPVttse2TbY7/fb8xTnQNiq42Svb7b5hzZaoNtj217FG+OzuXLl433G7WGhG8JAACQBjQfAADAKZoPAADgFM0HAABwiuYDAAA4RfMBAACcovkAAABOZe2cjyNHjsSd5fDss88a73v99dcb80WLFhlz24wN23natvPEEznP23aetW2NtjkdthkS58+fN+a29dnO9bedZ247V7+jo8OY2+Z0/Pvf/zbmtv1L5GdoU1xcbMxts1CKiopS+v6m51Auz/nw+Xzy+XzjZqnOt7HN0LDNyLDNELL9zCsrK425lPqMHduMCdv9bbXD9ry0zaex1Vfb/WfPnm3M8/PzjXlJSYkxLy8vN+a2WS+S9Pnnnxtz2ywpG9vzNN7xMyzecyyZupi7FQYAAOQkmg8AAOAUzQcAAHCK5gMAADhF8wEAAJyi+QAAAE7RfAAAAKeyds7HNddcE/d88K9//evG+x44cMCYt7S0THhdkv0c60AgYMxLS0utjzF//nxjbjvX3DavwDbv4OzZs8b8o48+Mua2eQjd3d3G3DZnYu/evcZ8xYoVxvy6664z5m+88YYxt+2vZD9X3mbWrFnG/OqrrzbmtudZf39/3Mw2KyGbFRQUqLCwcNyspqbGeN+BgQFjbpsfE41GjbltPs+JEyeMeSLPO9v8l8HBQWNu24N58+YZc9scjYKCAmNuq01f/vKXjXm8n/2wsrIyY247bv/zn/8Yc9tzoLq62phL5mNTss/TsM0JsT2PVq5caczjzeCy/exGSuqVj8bGRt18880KBAIqKyvTmjVrdPTo0TEP3tDQoMrKShUWFqq2tlaHDx9O5mEATDHUDgAjJdV8tLS0aMOGDdq3b5+ampo0ODiourq6UdMsn3rqKW3btk3bt2/X/v37FQqFdPvtt1un4gGYuqgdAEZK6s8uX3wpeufOnSorK1Nra6tWrlwpz/P09NNP67HHHtPatWslSc8//7zKy8u1a9cuPfDAA+lbOYCcQe0AMFJKbzgd/gyA4fcftLW1qbOzU3V1dbHb+P1+3XbbbXH/Rh+NRhWJREZdAExt1A5geptw8+F5njZv3qxbb71VS5YskfT/b8b64gfrlJeXx32jVmNjo4LBYOxSVVU10SUByAHUDgATbj4efvhhHTx4UH/4wx/GZF98t7DneXHfQbxlyxaFw+HYpb29faJLApADqB0AJnSq7caNG/Xqq69qz549WrhwYez6UCgk6b+/xVRUVMSu7+rqivsxw36/3/oRyQCmBmoHACnJ5sPzPG3cuFEvv/yympubx5wzX1NTo1AopKamJt14442S/nvOeEtLi5588smkFlZQUBD3fPAHH3wwqe/1Rbbz2G3ncX/88cfG/J133jHmn376qTGXpNbWVmN+/vx5Y24739p2LnteXp4xH/k/iPEM//zj+d73vmfMly1bZsxts1ZSZdu/Y8eOWb/HggULjLltHky8OTfDbHtgm6dgmndgu2+ysqV22OZ82GZU2OZw2I4b24wN26nFtvkNkn3Oha3+Df8pLB7b89Y248fGtr6vfvWrxtx23NjmlNga2uH3K8Vjm5EzNDRkzCXp1KlTxvzMmTPG3DaPxrbG5cuXG/N4z4Fk5nwkVcE3bNigXbt26S9/+YsCgUDsHxgMBlVYWCifz6dNmzZp69atWrx4sRYvXqytW7dq9uzZuu+++5J5KABTCLUDwEhJNR87duyQJNXW1o66fufOnVq/fr0k6dFHH9XFixf10EMP6dy5c1q+fLneeusta7cMYOqidgAYKek/u9j4fD41NDSooaFhomsCMMVQOwCMxAfLAQAAp2g+AACAUzQfAADAKZoPAADgFM0HAABwyuclMxXEgUgkomAwqHA4bB0WAyAzcvE4TMeao9FoSnlRUZExtw0hsw1/6uvrM+aSNGOG+XdK23C6y5cvG3PbEC7bv8H2v5z8/HxjPnfuXGM+FdiG0dk+RLG7u9uY9/f3G/PhD3yMZ3gi8XjruuqqqxI6BnnlAwAAOEXzAQAAnKL5AAAATtF8AAAAp2g+AACAUzQfAADAKZoPAADgVFKfagsAU5ltBkWmZ54UFhamlGeDOXPmTPYScp5tFottDodtnszQ0JAxnzVr1oS+v+1xR+KVDwAA4BTNBwAAcIrmAwAAOEXzAQAAnKL5AAAATtF8AAAAp2g+AACAU8z5AID/sc1XAHJBMBiclMdlzgcAAMhaNB8AAMApmg8AAOAUzQcAAHCK5gMAADhF8wEAAJyi+QAAAE4l1Xw0Njbq5ptvViAQUFlZmdasWaOjR4+Ous369evl8/lGXVasWJHWRQPILblSO2bMmGG8AEiPpI6mlpYWbdiwQfv27VNTU5MGBwdVV1envr6+Ube744471NHREbu8/vrraV00gNxC7QAwUlLj/N54441RX+/cuVNlZWVqbW3VypUrY9f7/X6FQqH0rBBAzqN2ABgppdcRw+GwJKmkpGTU9c3NzSorK9O1116r+++/X11dXXG/RzQaVSQSGXUBMLVRO4Dpzed5njeRO3qepzvvvFPnzp3Te++9F7t+9+7dmjNnjqqrq9XW1qaf/vSnGhwcVGtrq/x+/5jv09DQoJ/97Gdjrg+HwyouLp7I0gCkKBKJKBgMZuQ4zObaMTQ0ZMx53wcQXzJ1Y8LNx4YNG/Taa6/p/fff18KFC+PerqOjQ9XV1XrxxRe1du3aMXk0GlU0Gh21+KqqKpoPYBJlsvnI5tpB8wFMXDJ1Y0If4bhx40a9+uqr2rNnj7F4SFJFRYWqq6t17NixcXO/3z/ubzUAph5qBwApyebD8zxt3LhRL7/8spqbm1VTU2O9T09Pj9rb21VRUTHhRQLIbdQOACMl9Rrihg0b9Lvf/U67du1SIBBQZ2enOjs7dfHiRUnShQsX9Mgjj+gf//iHjh8/rubmZq1evVqlpaW66667MvIPAJD9cqV2DA4OGi8A0iOpVz527NghSaqtrR11/c6dO7V+/Xrl5eXp0KFDeuGFF3T+/HlVVFRo1apV2r17twKBQNoWDSC3UDsAjJT0n11MCgsL9eabb6a0IABTD7UDwEi8dRsAADhF8wEAAJyi+QAAAE7RfAAAAKdoPgAAgFMTmnAKAFNRfn7+ZC8BmBZ45QMAADhF8wEAAJyi+QAAAE7RfAAAAKdoPgAAgFM0HwAAwKmsO9V2+AOoIpHIJK8EmL6Gjz/bB8JlE2oHMLmSqRtZ13z09vZKkqqqqiZ5JQB6e3sVDAYnexkJoXYA2SGRuuHzsuxXm6GhIZ0+fVqBQEA+n0+RSERVVVVqb29XcXHxZC8vJ7GHqZtue+h5nnp7e1VZWakZM3Ljr7PUjvRjD1Mz3fYvmbqRda98zJgxQwsXLhxzfXFx8bT44WUSe5i66bSHufKKxzBqR+awh6mZTvuXaN3IjV9pAADAlEHzAQAAnMr65sPv9+vxxx+X3++f7KXkLPYwdexh7uFnljr2MDXsX3xZ94ZTAAAwtWX9Kx8AAGBqofkAAABO0XwAAACnaD4AAIBTNB8AAMCprG8+nn32WdXU1KigoEDLli3Te++9N9lLylp79uzR6tWrVVlZKZ/Pp1deeWVU7nmeGhoaVFlZqcLCQtXW1urw4cOTs9gs1NjYqJtvvlmBQEBlZWVas2aNjh49Ouo27GFuoG4kjrqRGurGxGR187F7925t2rRJjz32mA4cOKDvfOc7qq+v14kTJyZ7aVmpr69PN9xwg7Zv3z5u/tRTT2nbtm3avn279u/fr1AopNtvvz32gVzTXUtLizZs2KB9+/apqalJg4ODqqurU19fX+w27GH2o24kh7qRGurGBHlZ7Jvf/Kb34IMPjrruuuuu83784x9P0opyhyTv5Zdfjn09NDTkhUIh74knnohdd+nSJS8YDHq//OUvJ2GF2a+rq8uT5LW0tHiexx7mCurGxFE3UkfdSEzWvvIxMDCg1tZW1dXVjbq+rq5Oe/funaRV5a62tjZ1dnaO2k+/36/bbruN/YwjHA5LkkpKSiSxh7mAupFePOeTR91ITNY2H93d3bpy5YrKy8tHXV9eXq7Ozs5JWlXuGt4z9jMxnudp8+bNuvXWW7VkyRJJ7GEuoG6kF8/55FA3Ejdzshdg4/P5Rn3ted6Y65A49jMxDz/8sA4ePKj3339/TMYeZj9+RunFfiaGupG4rH3lo7S0VHl5eWM6w66urjEdJOxCoZAksZ8J2Lhxo1599VW9++67WrhwYex69jD7UTfSi+d84qgbycna5iM/P1/Lli1TU1PTqOubmpp0yy23TNKqcldNTY1CodCo/RwYGFBLSwv7+T+e5+nhhx/WSy+9pHfeeUc1NTWjcvYw+1E30ovnvB11Y4Im652uiXjxxRe9WbNmeb/97W+9I0eOeJs2bfKKioq848ePT/bSslJvb6934MAB78CBA54kb9u2bd6BAwe8zz77zPM8z3viiSe8YDDovfTSS96hQ4e8e++916uoqPAikcgkrzw7/OhHP/KCwaDX3NzsdXR0xC79/f2x27CH2Y+6kRzqRmqoGxOT1c2H53neM88841VXV3v5+fne0qVLY6cvYax3333XkzTmsm7dOs/z/nvK1+OPP+6FQiHP7/d7K1eu9A4dOjS5i84i4+2dJG/nzp2x27CHuYG6kTjqRmqoGxPj8zzPc/c6CwAAmO6y9j0fAABgaqL5AAAATtF8AAAAp2g+AACAUzQfAADAKZoPAADgFM0HAABwiuYDAAA4RfMBAACcovkAAABO0XwAAACn/g/xdSvaMY6NhAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(\"and the actual image looks like\")\n",
    "img_out = autoencoder.predict(test_images[0].reshape(1,28,28,1)) \n",
    "f, axarr = plt.subplots(1,2) \n",
    "axarr[0].imshow(test_images[0].reshape(28,28), cmap='Greys') \n",
    "axarr[1].imshow(img_out.reshape(28,28), cmap='Greys') \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('CoursIASDsurDL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61832d796b459ea0c328439f285fc36e83de2609730a347e1dc6879359cf168f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
